# UnifoLM-WMA å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### 1. å¿«é€Ÿå®‰è£…
```bash
# åˆ›å»ºç¯å¢ƒå¹¶å®‰è£…
conda create -n unifolm-wma python==3.10.18
conda activate unifolm-wma
conda install pinocchio=3.2.0 ffmpeg=7.1.1 -c conda-forge -y

# å…‹éš†å¹¶å®‰è£…é¡¹ç›®
git clone --recurse-submodules https://github.com/unitreerobotics/unifolm-world-model-action.git
cd unifolm-world-model-action
pip install -e .
cd external/dlimp && pip install -e . && cd ../..
```

### 2. å¿«é€Ÿä½“éªŒ - æŸ¥çœ‹æ¶æ„ä¿¡æ¯
```bash
python examples/usage_example.py --mode info
```

### 3. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
```bash
# ä¸‹è½½ UnifoLM-WMA-0_Base æ¨¡å‹
# ä» HuggingFace: https://huggingface.co/unitreerobotics/UnifoLM-WMA-0
# ä¿å­˜åˆ°: models/unifolm-wma-base.ckpt
```

### 4. å‡†å¤‡ä½ çš„æ•°æ® (å¯é€‰ - ä½¿ç”¨è‡ªå·±çš„æ•°æ®)
```bash
python examples/usage_example.py --mode prepare_data \
    --source_dir /path/to/your/raw/data \
    --target_dir /path/to/processed/data \
    --dataset_name my_robot_task
```

### 5. å¼€å§‹è®­ç»ƒ
```bash
# ç¼–è¾‘é…ç½®æ–‡ä»¶
# 1. æ›´æ–° configs/train/config.yaml ä¸­çš„æ•°æ®è·¯å¾„
# 2. è®¾ç½®é¢„è®­ç»ƒæ£€æŸ¥ç‚¹è·¯å¾„
# 3. æ ¹æ®ä½ çš„æœºå™¨äººè°ƒæ•´ agent_state_dim å’Œ agent_action_dim

# å¼€å§‹è®­ç»ƒ
python examples/usage_example.py --mode train \
    --config configs/train/config.yaml \
    --name my_first_experiment
```

### 6. è¿è¡Œæ¨ç†
```bash
python examples/usage_example.py --mode inference \
    --ckpt_path /path/to/your/trained/model.ckpt \
    --prompt_dir examples/ \
    --dataset my_robot_task
```

## ğŸ“Š å…¸å‹è®­ç»ƒæ—¶é—´å’Œèµ„æºéœ€æ±‚

| åœºæ™¯ | GPU | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜éœ€æ±‚ |
|------|-----|----------|----------|
| å°å‹æ•°æ®é›† (1Kè½¨è¿¹) | 1Ã—RTX 3090 | 2-4å°æ—¶ | 12GB |
| ä¸­å‹æ•°æ®é›† (10Kè½¨è¿¹) | 2Ã—RTX 4090 | 8-12å°æ—¶ | 24GBÃ—2 |
| å¤§å‹æ•°æ®é›† (100Kè½¨è¿¹) | 8Ã—A100 | 1-2å¤© | 40GBÃ—8 |

## ğŸ¯ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: æœºæ¢°è‡‚æŠ“å–ä»»åŠ¡
```yaml
# configs/train/config.yaml
model:
  params:
    agent_state_dim: 7     # 7è‡ªç”±åº¦æœºæ¢°è‡‚
    agent_action_dim: 7    # å…³èŠ‚è§’åº¦æ§åˆ¶
    decision_making_only: true
```

### åœºæ™¯2: ç§»åŠ¨æœºå™¨äººå¯¼èˆª
```yaml
# configs/train/config.yaml  
model:
  params:
    agent_state_dim: 3     # x, y, theta
    agent_action_dim: 2    # çº¿é€Ÿåº¦, è§’é€Ÿåº¦
    decision_making_only: true
```

### åœºæ™¯3: åŒè‡‚åä½œä»»åŠ¡
```yaml
# configs/train/config.yaml
model:
  params:
    agent_state_dim: 14    # 7DOF Ã— 2è‡‚
    agent_action_dim: 14   # åŒè‡‚è”åˆæ§åˆ¶
    decision_making_only: false  # å¯ç”¨ä»¿çœŸæ¨¡å¼
```

## ğŸ”§ å¿«é€Ÿé…ç½®æ£€æŸ¥æ¸…å•

è®­ç»ƒå‰æ£€æŸ¥ä»¥ä¸‹é…ç½®:

### âœ… æ•°æ®é…ç½®
- [ ] æ•°æ®è·¯å¾„æ­£ç¡®: `data.params.train.params.data_dir`
- [ ] æ•°æ®é›†åç§°åŒ¹é…: `data.params.dataset_and_weights`
- [ ] meta.json ç»´åº¦æ­£ç¡®: `obs.agent_pos.shape` å’Œ `action.shape`

### âœ… æ¨¡å‹é…ç½®  
- [ ] é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å­˜åœ¨: `model.pretrained_checkpoint`
- [ ] çŠ¶æ€ç»´åº¦æ­£ç¡®: `model.params.agent_state_dim`
- [ ] åŠ¨ä½œç»´åº¦æ­£ç¡®: `model.params.agent_action_dim`
- [ ] å›¾åƒå°ºå¯¸åˆç†: `data.params.train.params.resolution`

### âœ… èµ„æºé…ç½®
- [ ] GPUå†…å­˜è¶³å¤Ÿ (è‡³å°‘8GB)
- [ ] å­˜å‚¨ç©ºé—´è¶³å¤Ÿ (æ¨¡å‹+æ—¥å¿—+æ£€æŸ¥ç‚¹)
- [ ] CPUå’Œå†…å­˜é€‚å½“: `data.params.num_workers`

## ğŸ› å¸¸è§é—®é¢˜å¿«é€Ÿè§£å†³

### é—®é¢˜1: å†…å­˜ä¸è¶³ (CUDA out of memory)
```yaml
# è§£å†³æ–¹æ¡ˆ: å‡å°‘æ‰¹å¤§å°å’Œè§†é¢‘é•¿åº¦
data:
  params:
    batch_size: 1
    train:
      params:
        video_length: 8
        num_workers: 4
```

### é—®é¢˜2: æ•°æ®åŠ è½½é”™è¯¯
```bash
# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
python -c "
import h5py
f = h5py.File('transitions/dataset_name/0.h5', 'r')
print('Keys:', list(f.keys()))
print('Observation keys:', list(f['observation'].keys()))
print('Action shape:', f['action'].shape)
"
```

### é—®é¢˜3: è®­ç»ƒæŸå¤±ä¸ä¸‹é™
```yaml
# è§£å†³æ–¹æ¡ˆ: è°ƒæ•´å­¦ä¹ ç‡å’Œé¢„å¤„ç†
model:
  base_learning_rate: 5.0e-06  # é™ä½å­¦ä¹ ç‡
  params:
    uncond_prob: 0.1            # å¢åŠ æ— æ¡ä»¶è®­ç»ƒ
    input_pertub: 0.05          # å‡å°‘è¾“å…¥æ‰°åŠ¨
```

### é—®é¢˜4: æ¨ç†ç»“æœè´¨é‡å·®
```yaml
# è§£å†³æ–¹æ¡ˆ: è°ƒæ•´æ¨ç†å‚æ•°
inference:
  ddim_steps: 100              # å¢åŠ é‡‡æ ·æ­¥æ•°
  guidance_scale: 10.0         # å¢åŠ å¼•å¯¼å¼ºåº¦
  ddim_eta: 0.1               # è°ƒæ•´éšæœºæ€§
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒåŠ é€Ÿ
1. **ä½¿ç”¨æ··åˆç²¾åº¦**: åœ¨é…ç½®ä¸­è®¾ç½® `precision: 16`
2. **å¤šGPUè®­ç»ƒ**: ä½¿ç”¨ `torchrun --nproc_per_node=N`
3. **æ•°æ®é¢„åŠ è½½**: å¢åŠ  `num_workers` æ•°é‡
4. **æ¢¯åº¦ç´¯ç§¯**: è®¾ç½® `gradient_accumulate_every: 2`

### æ¨ç†åŠ é€Ÿ
1. **å‡å°‘DDIMæ­¥æ•°**: `ddim_steps: 25-50`
2. **æ‰¹é‡æ¨ç†**: åŒæ—¶å¤„ç†å¤šä¸ªæ ·æœ¬
3. **æ¨¡å‹é‡åŒ–**: ä½¿ç”¨torch.jitæˆ–TensorRT
4. **ç¼“å­˜ä¼˜åŒ–**: é¢„åŠ è½½æ¨¡å‹æƒé‡

## ğŸ“ è¿›é˜¶å­¦ä¹ è·¯å¾„

1. **ç†è§£æ¶æ„**: é˜…è¯» `ARCHITECTURE_ANALYSIS.md`
2. **æ·±å…¥é…ç½®**: å­¦ä¹  `CONFIG_GUIDE.md`  
3. **æŸ¥çœ‹æºç **: ä» `src/unifolm_wma/models/ddpms.py` å¼€å§‹
4. **å®éªŒå¯¹æ¯”**: å°è¯•ä¸åŒçš„æ¨¡å‹é…ç½®å’Œè®­ç»ƒç­–ç•¥
5. **ç¤¾åŒºäº¤æµ**: å‚ä¸GitHub Issueså’ŒDiscussions

## ğŸ“ è·å–å¸®åŠ©

- **æ–‡æ¡£**: æŸ¥çœ‹READMEå’Œé…ç½®æŒ‡å—
- **ç¤ºä¾‹**: è¿è¡Œ `examples/usage_example.py`
- **Issues**: åœ¨GitHubä¸Šæäº¤é—®é¢˜
- **è®ºæ–‡**: é˜…è¯»ç›¸å…³æŠ€æœ¯è®ºæ–‡äº†è§£åŸç†

---

ğŸ‰ **æ­å–œ!** ä½ å·²ç»æŒæ¡äº†UnifoLM-WMAçš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•ã€‚ç°åœ¨å¼€å§‹æ„å»ºä½ çš„æœºå™¨äººä¸–ç•Œæ¨¡å‹å§!